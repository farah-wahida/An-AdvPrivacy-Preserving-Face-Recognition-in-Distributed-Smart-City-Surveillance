{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup and Dataset Download"
      ],
      "metadata": {
        "id": "hZPrRqDSnTta"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJjIjx_bnOHJ"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow_federated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"mewkhi\",\"key\":\"51a23160bc635731f18e0b9a7bc75b53\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d hereisburak/pins-face-recognition\n",
        "!mkdir PinsData\n",
        "!unzip -q pins-face-recognition.zip -d ./PinsData"
      ],
      "metadata": {
        "id": "ssV4bhfrnW84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Noise on Raw Dataset to Generate Perturbed Dataset"
      ],
      "metadata": {
        "id": "k4gFG6blnZSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/content/PinsData/105_classes_pins_dataset\"\n",
        "\n",
        "# Define the transform for the DataLoader used for visualization (with normalization)\n",
        "transform_visualize = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create the ImageFolder dataset with normalization\n",
        "visualize_dataset = ImageFolder(dataset_path, transform=transform_visualize)\n",
        "\n",
        "# Define the network architecture\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "learning_rate = 0.004\n",
        "\n",
        "# Initialize the global model\n",
        "global_model = AlexNet().to(device)\n",
        "\n",
        "# Define the optimizer and loss function for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ImageFolder(dataset_path, transform=transform_visualize)\n",
        "\n",
        "# Create a directory to save the perturbed images\n",
        "perturbed_dir = \"perturbed_images\"\n",
        "os.makedirs(perturbed_dir, exist_ok=True)\n",
        "\n",
        "# Define the epsilon value for the DeepFool attack\n",
        "epsilon = 0.03\n",
        "\n",
        "# Modified DeepFool attack to increase perturbation\n",
        "def deepfool_attack_batch(images, model, epsilon, reduction_factor=0.95):\n",
        "    model.eval()\n",
        "    perturbed_images = images.clone().detach().requires_grad_(True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        original_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    outputs_grad = None\n",
        "    while True:\n",
        "        outputs = model(perturbed_images)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        if torch.all(predicted_labels == original_labels):\n",
        "            break\n",
        "\n",
        "        if perturbed_images.grad is not None:\n",
        "            perturbed_images.grad.data.zero_()\n",
        "\n",
        "        loss = criterion(outputs, original_labels)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        if perturbed_images.grad is not None and outputs_grad is None:\n",
        "            outputs_grad = perturbed_images.grad.data.clone()\n",
        "\n",
        "        if outputs_grad is not None:\n",
        "            perturbation = torch.abs(loss) / torch.norm(outputs_grad.view(images.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
        "            perturbed_images = perturbed_images + perturbation * outputs_grad / torch.norm(outputs_grad.view(images.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
        "            perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "            perturbed_images = perturbed_images.clone().detach().requires_grad_(True).to(device)\n",
        "        else:\n",
        "            raise ValueError(\"No gradient computed for perturbed_images. The attack can't proceed.\")\n",
        "\n",
        "    return perturbed_images.detach()\n",
        "\n",
        "# Apply the modified DeepFool attack to the entire dataset and save the perturbed images\n",
        "batch_size = 64\n",
        "total_images = len(visualize_dataset)\n",
        "num_batches = (total_images + batch_size - 1) // batch_size\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min(start_idx + batch_size, total_images)\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = zip(*[visualize_dataset[i] for i in range(start_idx, end_idx)])\n",
        "    images = torch.stack(images).to(device)\n",
        "    labels = torch.tensor(labels).to(device)\n",
        "\n",
        "    # DeepFool attack for the batch of images\n",
        "    perturbed_images = deepfool_attack_batch(images, global_model, epsilon)\n",
        "\n",
        "    for j in range(end_idx - start_idx):\n",
        "        # Convert the perturbed image to a numpy array and change the channel order\n",
        "        perturbed_image_np = perturbed_images[j].squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "        # Convert the image to the BGR color format for saving\n",
        "        perturbed_image_bgr = cv2.cvtColor((perturbed_image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Get the class label of the image\n",
        "        class_label = visualize_dataset.classes[labels[j]]\n",
        "\n",
        "        # Create a subdirectory for the class label if it does not exist\n",
        "        class_subdir = os.path.join(perturbed_dir, class_label)\n",
        "        os.makedirs(class_subdir, exist_ok=True)\n",
        "\n",
        "        # Save the perturbed image in the class subdirectory\n",
        "        perturbed_image_path = os.path.join(class_subdir, f\"perturbed_{start_idx + j}.jpg\")\n",
        "        cv2.imwrite(perturbed_image_path, perturbed_image_bgr)\n",
        "\n",
        "        # Undo normalization on the original image\n",
        "        original_image_np = images[j].squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
        "        original_image_np = (original_image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
        "        original_image_np = original_image_np.astype(np.uint8)\n",
        "\n",
        "        # Convert the image to the BGR color format for visualization\n",
        "        original_image_bgr = cv2.cvtColor(original_image_np, cv2.COLOR_RGB2BGR)"
      ],
      "metadata": {
        "id": "95XH7TcsndwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Federated Learning Environment (using ResNet18 model)"
      ],
      "metadata": {
        "id": "ajdsqn8Lnhww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/perturbed_images/\"\n",
        "\n",
        "# Define the network architecture (using ResNet18)\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.model = resnet18(pretrained=True)\n",
        "        num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Wrap the model with DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallelism.\")\n",
        "    global_model = nn.DataParallel(ResNet18()).to(device)\n",
        "else:\n",
        "    global_model = ResNet18().to(device)\n",
        "\n",
        "# Define the federated learning parameters\n",
        "num_clients = 4\n",
        "fraction = 0.2\n",
        "num_epochs = 150\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the optimizer and learning rate scheduler for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Divide the dataset into train and test sets\n",
        "full_dataset = ImageFolder(dataset_path, transform=transform)\n",
        "num_samples = len(full_dataset)\n",
        "num_train_samples = int(num_samples * (1 - fraction))\n",
        "num_test_samples = num_samples - num_train_samples\n",
        "train_dataset, test_dataset = random_split(full_dataset, [num_train_samples, num_test_samples])\n",
        "\n",
        "# Split the train dataset into client datasets\n",
        "train_indices = list(range(len(train_dataset)))\n",
        "client_datasets = []\n",
        "start = 0\n",
        "for _ in range(num_clients):\n",
        "    end = start + int(len(train_indices) / num_clients)\n",
        "    indices = train_indices[start:end]\n",
        "    subset = Subset(train_dataset, indices)\n",
        "    client_datasets.append(subset)\n",
        "    start = end\n",
        "\n",
        "# Function to train a local model on a client's dataset\n",
        "def train_local_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct / total * 100\n",
        "    return model.state_dict(), train_loss, train_accuracy\n",
        "\n",
        "# Function to aggregate model updates using federated averaging\n",
        "def aggregate_models(global_model, local_models):\n",
        "    global_dict = global_model.state_dict()\n",
        "    for key in global_dict.keys():\n",
        "        # Convert model weights to float\n",
        "        global_dict[key] = torch.stack([local_models[i][key].float() for i in range(len(local_models))], dim=0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Lists to store the values for plotting\n",
        "train_losses_plot = []\n",
        "train_accuracies_plot = []\n",
        "test_losses_plot = []\n",
        "test_accuracies_plot = []\n",
        "\n",
        "# Train the global model using federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    local_models = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    # Train local models on client datasets\n",
        "    for client_dataset in client_datasets:\n",
        "        train_loader = DataLoader(client_dataset, batch_size=32, shuffle=True)\n",
        "        local_model = ResNet18().to(device)\n",
        "        # Wrap the local model with DataParallel if multiple GPUs are available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "                local_model = nn.DataParallel(local_model)\n",
        "\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "        local_optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Learning rate scheduler for local models\n",
        "        local_scheduler = optim.lr_scheduler.StepLR(local_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "        local_model_dict, train_loss, train_accuracy = train_local_model(local_model, train_loader, local_optimizer, criterion, device)\n",
        "        local_models.append(local_model_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        local_scheduler.step()\n",
        "\n",
        "    # Aggregate the local models using federated averaging\n",
        "    global_model = aggregate_models(global_model, local_models)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    global_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = global_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = test_correct / test_total * 100\n",
        "\n",
        "    # Append values to the plotting lists\n",
        "    train_losses_plot.append(sum(train_losses) / len(train_losses))\n",
        "    train_accuracies_plot.append(sum(train_accuracies) / len(train_accuracies))\n",
        "    test_losses_plot.append(test_loss)\n",
        "    test_accuracies_plot.append(test_accuracy)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {sum(train_losses) / len(train_losses):.4f} | Train Accuracy: {sum(train_accuracies) / len(train_accuracies):.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plotting train and test loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_plot, label='Train Loss')\n",
        "plt.plot(test_losses_plot, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting train and test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies_plot, label='Train Accuracy')\n",
        "plt.plot(test_accuracies_plot, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8PmziXA2niLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Federated Learning Environment (using GoogleNet model)"
      ],
      "metadata": {
        "id": "z7zkzdKYoHk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.models import googlenet\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/perturbed_images/\"\n",
        "\n",
        "# Define the network architecture (GoogLeNet)\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        # Load the pretrained GoogLeNet model\n",
        "        self.model = googlenet(pretrained=True)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes\n",
        "        self.model.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Wrap the model with DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallelism.\")\n",
        "    global_model = nn.DataParallel(GoogLeNet()).to(device)\n",
        "else:\n",
        "    global_model = GoogLeNet().to(device)\n",
        "\n",
        "# Define the federated learning parameters\n",
        "num_clients = 4\n",
        "fraction = 0.2\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the optimizer and learning rate scheduler for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Divide the dataset into train and test sets\n",
        "full_dataset = ImageFolder(dataset_path, transform=transform)\n",
        "num_samples = len(full_dataset)\n",
        "num_train_samples = int(num_samples * (1 - fraction))\n",
        "num_test_samples = num_samples - num_train_samples\n",
        "train_dataset, test_dataset = random_split(full_dataset, [num_train_samples, num_test_samples])\n",
        "\n",
        "# Split the train dataset into client datasets\n",
        "train_indices = list(range(len(train_dataset)))\n",
        "client_datasets = []\n",
        "start = 0\n",
        "for _ in range(num_clients):\n",
        "    end = start + int(len(train_indices) / num_clients)\n",
        "    indices = train_indices[start:end]\n",
        "    subset = Subset(train_dataset, indices)\n",
        "    client_datasets.append(subset)\n",
        "    start = end\n",
        "\n",
        "# Function to train a local model on a client's dataset\n",
        "def train_local_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct / total * 100\n",
        "    return model.state_dict(), train_loss, train_accuracy\n",
        "\n",
        "# Function to aggregate model updates using federated averaging\n",
        "def aggregate_models(global_model, local_models):\n",
        "    global_dict = global_model.state_dict()\n",
        "    for key in global_dict.keys():\n",
        "        # Convert model weights to float\n",
        "        global_dict[key] = torch.stack([local_models[i][key].float() for i in range(len(local_models))], dim=0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Lists to store the values for plotting\n",
        "train_losses_plot = []\n",
        "train_accuracies_plot = []\n",
        "test_losses_plot = []\n",
        "test_accuracies_plot = []\n",
        "\n",
        "# Train the global model using federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    local_models = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    # Train local models on client datasets\n",
        "    for client_dataset in client_datasets:\n",
        "        train_loader = DataLoader(client_dataset, batch_size=32, shuffle=True)\n",
        "        local_model = GoogLeNet().to(device)\n",
        "        # Wrap the local model with DataParallel if multiple GPUs are available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "                local_model = nn.DataParallel(local_model)\n",
        "\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "        local_optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Learning rate scheduler for local models\n",
        "        local_scheduler = optim.lr_scheduler.StepLR(local_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "        local_model_dict, train_loss, train_accuracy = train_local_model(local_model, train_loader, local_optimizer, criterion, device)\n",
        "        local_models.append(local_model_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        local_scheduler.step()\n",
        "\n",
        "    # Aggregate the local models using federated averaging\n",
        "    global_model = aggregate_models(global_model, local_models)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    global_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = global_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = test_correct / test_total * 100\n",
        "\n",
        "    # Append values to the plotting lists\n",
        "    train_losses_plot.append(sum(train_losses) / len(train_losses))\n",
        "    train_accuracies_plot.append(sum(train_accuracies) / len(train_accuracies))\n",
        "    test_losses_plot.append(test_loss)\n",
        "    test_accuracies_plot.append(test_accuracy)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {sum(train_losses) / len(train_losses):.4f} | Train Accuracy: {sum(train_accuracies) / len(train_accuracies):.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plotting train and test loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_plot, label='Train Loss')\n",
        "plt.plot(test_losses_plot, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting train and test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies_plot, label='Train Accuracy')\n",
        "plt.plot(test_accuracies_plot, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fhht_DL0oNNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Federated Learning Environment (using DenseNet model)"
      ],
      "metadata": {
        "id": "5FHXEfkjoW4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/perturbed_images/\"\n",
        "\n",
        "# Define the network architecture (DenseNet)\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(DenseNet, self).__init__()\n",
        "        # Load the pretrained DenseNet model\n",
        "        self.model = models.densenet121(pretrained=True)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes\n",
        "        num_ftrs = self.model.classifier.in_features\n",
        "        self.model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Wrap the model with DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallelism.\")\n",
        "    global_model = nn.DataParallel(DenseNet()).to(device)\n",
        "else:\n",
        "    global_model = DenseNet().to(device)\n",
        "\n",
        "# Define the federated learning parameters\n",
        "num_clients = 4\n",
        "fraction = 0.2\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the optimizer and learning rate scheduler for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Divide the dataset into train and test sets\n",
        "full_dataset = ImageFolder(dataset_path, transform=transform)\n",
        "num_samples = len(full_dataset)\n",
        "num_train_samples = int(num_samples * (1 - fraction))\n",
        "num_test_samples = num_samples - num_train_samples\n",
        "train_dataset, test_dataset = random_split(full_dataset, [num_train_samples, num_test_samples])\n",
        "\n",
        "# Split the train dataset into client datasets\n",
        "train_indices = list(range(len(train_dataset)))\n",
        "client_datasets = []\n",
        "start = 0\n",
        "for _ in range(num_clients):\n",
        "    end = start + int(len(train_indices) / num_clients)\n",
        "    indices = train_indices[start:end]\n",
        "    subset = Subset(train_dataset, indices)\n",
        "    client_datasets.append(subset)\n",
        "    start = end\n",
        "\n",
        "# Function to train a local model on a client's dataset\n",
        "def train_local_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct / total * 100\n",
        "    return model.state_dict(), train_loss, train_accuracy\n",
        "\n",
        "# Function to aggregate model updates using federated averaging\n",
        "def aggregate_models(global_model, local_models):\n",
        "    global_dict = global_model.state_dict()\n",
        "    for key in global_dict.keys():\n",
        "        # Convert model weights to float\n",
        "        global_dict[key] = torch.stack([local_models[i][key].float() for i in range(len(local_models))], dim=0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Lists to store the values for plotting\n",
        "train_losses_plot = []\n",
        "train_accuracies_plot = []\n",
        "test_losses_plot = []\n",
        "test_accuracies_plot = []\n",
        "\n",
        "# Train the global model using federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    local_models = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    # Train local models on client datasets\n",
        "    for client_dataset in client_datasets:\n",
        "        train_loader = DataLoader(client_dataset, batch_size=32, shuffle=True)\n",
        "        local_model = GoogLeNet().to(device)\n",
        "        # Wrap the local model with DataParallel if multiple GPUs are available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "                local_model = nn.DataParallel(local_model)\n",
        "\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "        local_optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Learning rate scheduler for local models\n",
        "        local_scheduler = optim.lr_scheduler.StepLR(local_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "        local_model_dict, train_loss, train_accuracy = train_local_model(local_model, train_loader, local_optimizer, criterion, device)\n",
        "        local_models.append(local_model_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        local_scheduler.step()\n",
        "\n",
        "    # Aggregate the local models using federated averaging\n",
        "    global_model = aggregate_models(global_model, local_models)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    global_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = global_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = test_correct / test_total * 100\n",
        "\n",
        "    # Append values to the plotting lists\n",
        "    train_losses_plot.append(sum(train_losses) / len(train_losses))\n",
        "    train_accuracies_plot.append(sum(train_accuracies) / len(train_accuracies))\n",
        "    test_losses_plot.append(test_loss)\n",
        "    test_accuracies_plot.append(test_accuracy)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {sum(train_losses) / len(train_losses):.4f} | Train Accuracy: {sum(train_accuracies) / len(train_accuracies):.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plotting train and test loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_plot, label='Train Loss')\n",
        "plt.plot(test_losses_plot, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting train and test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies_plot, label='Train Accuracy')\n",
        "plt.plot(test_accuracies_plot, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-35XVuMZocUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Federated Learning Environment (using MobileNet model)"
      ],
      "metadata": {
        "id": "zn28yMMl1XhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/perturbed_images/\"\n",
        "\n",
        "# Define the network architecture (MobileNet)\n",
        "class MobileNet(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(MobileNet, self).__init__()\n",
        "        # Load the pretrained MobileNet model\n",
        "        self.model = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes\n",
        "        num_ftrs = self.model.classifier[1].in_features\n",
        "        self.model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Wrap the model with DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallelism.\")\n",
        "    global_model = nn.DataParallel(MobileNet()).to(device)\n",
        "else:\n",
        "    global_model = MobileNet().to(device)\n",
        "\n",
        "# Define the federated learning parameters\n",
        "num_clients = 4\n",
        "fraction = 0.2\n",
        "num_epochs = 300\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the optimizer and learning rate scheduler for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is often used for classification\n",
        "\n",
        "# Divide the dataset into train and test sets\n",
        "full_dataset = ImageFolder(dataset_path, transform=transform)\n",
        "num_samples = len(full_dataset)\n",
        "num_train_samples = int(num_samples * (1 - fraction))\n",
        "num_test_samples = num_samples - num_train_samples\n",
        "train_dataset, test_dataset = random_split(full_dataset, [num_train_samples, num_test_samples])\n",
        "\n",
        "# Split the train dataset into client datasets\n",
        "train_indices = list(range(len(train_dataset)))\n",
        "#random.shuffle(train_indices)\n",
        "client_datasets = []\n",
        "start = 0\n",
        "for _ in range(num_clients):\n",
        "    end = start + int(len(train_indices) / num_clients)\n",
        "    indices = train_indices[start:end]\n",
        "    subset = Subset(train_dataset, indices)\n",
        "    client_datasets.append(subset)\n",
        "    start = end\n",
        "\n",
        "# Function to train a local model on a client's dataset\n",
        "def train_local_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct / total * 100\n",
        "    return model.state_dict(), train_loss, train_accuracy\n",
        "\n",
        "# Function to aggregate model updates using federated averaging\n",
        "def aggregate_models(global_model, local_models):\n",
        "    global_dict = global_model.state_dict()\n",
        "    for key in global_dict.keys():\n",
        "        # Convert model weights to float\n",
        "        global_dict[key] = torch.stack([local_models[i][key].float() for i in range(len(local_models))], dim=0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Lists to store the values for plotting\n",
        "train_losses_plot = []\n",
        "train_accuracies_plot = []\n",
        "test_losses_plot = []\n",
        "test_accuracies_plot = []\n",
        "\n",
        "# Train the global model using federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    local_models = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    # Train local models on client datasets\n",
        "    for client_dataset in client_datasets:\n",
        "        train_loader = DataLoader(client_dataset, batch_size=32, shuffle=True)\n",
        "        local_model = GoogLeNet().to(device)\n",
        "        # Wrap the local model with DataParallel if multiple GPUs are available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "                local_model = nn.DataParallel(local_model)\n",
        "\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "        local_optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Learning rate scheduler for local models\n",
        "        local_scheduler = optim.lr_scheduler.StepLR(local_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "        local_model_dict, train_loss, train_accuracy = train_local_model(local_model, train_loader, local_optimizer, criterion, device)\n",
        "        local_models.append(local_model_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        local_scheduler.step()\n",
        "\n",
        "    # Aggregate the local models using federated averaging\n",
        "    global_model = aggregate_models(global_model, local_models)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    global_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = global_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = test_correct / test_total * 100\n",
        "\n",
        "    # Append values to the plotting lists\n",
        "    train_losses_plot.append(sum(train_losses) / len(train_losses))\n",
        "    train_accuracies_plot.append(sum(train_accuracies) / len(train_accuracies))\n",
        "    test_losses_plot.append(test_loss)\n",
        "    test_accuracies_plot.append(test_accuracy)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {sum(train_losses) / len(train_losses):.4f} | Train Accuracy: {sum(train_accuracies) / len(train_accuracies):.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plotting train and test loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_plot, label='Train Loss')\n",
        "plt.plot(test_losses_plot, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting train and test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies_plot, label='Train Accuracy')\n",
        "plt.plot(test_accuracies_plot, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6RhFzBsm1aaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Federated Learning Environment (using ResNeXt model)"
      ],
      "metadata": {
        "id": "WkQsaPu42YYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/perturbed_images/\"\n",
        "\n",
        "# Define the network architecture (ResNeXt)\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        # Load the pretrained ResNeXt model\n",
        "        self.model = models.resnext50_32x4d(pretrained=True)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes\n",
        "        self.model.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Wrap the model with DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallelism.\")\n",
        "    global_model = nn.DataParallel(ResNeXt()).to(device)\n",
        "else:\n",
        "    global_model = ResNeXt().to(device)\n",
        "\n",
        "# Define the federated learning parameters\n",
        "num_clients = 4\n",
        "fraction = 0.2\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the optimizer and learning rate scheduler for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Divide the dataset into train and test sets\n",
        "full_dataset = ImageFolder(dataset_path, transform=transform)\n",
        "num_samples = len(full_dataset)\n",
        "num_train_samples = int(num_samples * (1 - fraction))\n",
        "num_test_samples = num_samples - num_train_samples\n",
        "train_dataset, test_dataset = random_split(full_dataset, [num_train_samples, num_test_samples])\n",
        "\n",
        "# Split the train dataset into client datasets\n",
        "train_indices = list(range(len(train_dataset)))\n",
        "client_datasets = []\n",
        "start = 0\n",
        "for _ in range(num_clients):\n",
        "    end = start + int(len(train_indices) / num_clients)\n",
        "    indices = train_indices[start:end]\n",
        "    subset = Subset(train_dataset, indices)\n",
        "    client_datasets.append(subset)\n",
        "    start = end\n",
        "\n",
        "# Function to train a local model on a client's dataset\n",
        "def train_local_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct / total * 100\n",
        "    return model.state_dict(), train_loss, train_accuracy\n",
        "\n",
        "# Function to aggregate model updates using federated averaging\n",
        "def aggregate_models(global_model, local_models):\n",
        "    global_dict = global_model.state_dict()\n",
        "    for key in global_dict.keys():\n",
        "        # Convert model weights to float\n",
        "        global_dict[key] = torch.stack([local_models[i][key].float() for i in range(len(local_models))], dim=0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Lists to store the values for plotting\n",
        "train_losses_plot = []\n",
        "train_accuracies_plot = []\n",
        "test_losses_plot = []\n",
        "test_accuracies_plot = []\n",
        "\n",
        "# Train the global model using federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    local_models = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    # Train local models on client datasets\n",
        "    for client_dataset in client_datasets:\n",
        "        train_loader = DataLoader(client_dataset, batch_size=32, shuffle=True)\n",
        "        local_model = GoogLeNet().to(device)\n",
        "        # Wrap the local model with DataParallel if multiple GPUs are available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "                local_model = nn.DataParallel(local_model)\n",
        "\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "        local_optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Learning rate scheduler for local models\n",
        "        local_scheduler = optim.lr_scheduler.StepLR(local_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "        local_model_dict, train_loss, train_accuracy = train_local_model(local_model, train_loader, local_optimizer, criterion, device)\n",
        "        local_models.append(local_model_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        local_scheduler.step()\n",
        "\n",
        "    # Aggregate the local models using federated averaging\n",
        "    global_model = aggregate_models(global_model, local_models)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    global_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = global_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = test_correct / test_total * 100\n",
        "\n",
        "    # Append values to the plotting lists\n",
        "    train_losses_plot.append(sum(train_losses) / len(train_losses))\n",
        "    train_accuracies_plot.append(sum(train_accuracies) / len(train_accuracies))\n",
        "    test_losses_plot.append(test_loss)\n",
        "    test_accuracies_plot.append(test_accuracy)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {sum(train_losses) / len(train_losses):.4f} | Train Accuracy: {sum(train_accuracies) / len(train_accuracies):.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plotting train and test loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_plot, label='Train Loss')\n",
        "plt.plot(test_losses_plot, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting train and test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies_plot, label='Train Accuracy')\n",
        "plt.plot(test_accuracies_plot, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8MKrlKNQ2gBV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}