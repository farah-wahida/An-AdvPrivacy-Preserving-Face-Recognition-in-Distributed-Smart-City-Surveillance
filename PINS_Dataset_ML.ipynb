{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup and Dataset Download"
      ],
      "metadata": {
        "id": "Rj_WoKeilCyz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ROlw9hsk0xD"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow_federated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"username\":\"mewkhi\",\"key\":\"51a23160bc635731f18e0b9a7bc75b53\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d hereisburak/pins-face-recognition\n",
        "!mkdir PinsData\n",
        "!unzip -q pins-face-recognition.zip -d ./PinsData"
      ],
      "metadata": {
        "id": "CU1ATRJClHh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying Noise on Raw Dataset to Generate Perturbed Dataset"
      ],
      "metadata": {
        "id": "8-b3kCkhlKzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/content/PinsData/105_classes_pins_dataset\"\n",
        "\n",
        "# Define the transform for the DataLoader used for visualization (with normalization)\n",
        "transform_visualize = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create the ImageFolder dataset with normalization\n",
        "visualize_dataset = ImageFolder(dataset_path, transform=transform_visualize)\n",
        "\n",
        "# Define the network architecture\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "learning_rate = 0.004\n",
        "\n",
        "# Initialize the global model\n",
        "global_model = AlexNet().to(device)\n",
        "\n",
        "# Define the optimizer and loss function for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ImageFolder(dataset_path, transform=transform_visualize)\n",
        "\n",
        "# Create a directory to save the perturbed images\n",
        "perturbed_dir = \"perturbed_images\"\n",
        "os.makedirs(perturbed_dir, exist_ok=True)\n",
        "\n",
        "# Define the epsilon value for the DeepFool attack\n",
        "epsilon = 0.03\n",
        "\n",
        "# Modified DeepFool attack to increase perturbation\n",
        "def deepfool_attack_batch(images, model, epsilon, reduction_factor=0.95):\n",
        "    model.eval()\n",
        "    perturbed_images = images.clone().detach().requires_grad_(True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        original_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    outputs_grad = None\n",
        "    while True:\n",
        "        outputs = model(perturbed_images)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        if torch.all(predicted_labels == original_labels):\n",
        "            break\n",
        "\n",
        "        if perturbed_images.grad is not None:\n",
        "            perturbed_images.grad.data.zero_()\n",
        "\n",
        "        loss = criterion(outputs, original_labels)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        if perturbed_images.grad is not None and outputs_grad is None:\n",
        "            outputs_grad = perturbed_images.grad.data.clone()\n",
        "\n",
        "        if outputs_grad is not None:\n",
        "            perturbation = torch.abs(loss) / torch.norm(outputs_grad.view(images.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
        "            perturbed_images = perturbed_images + perturbation * outputs_grad / torch.norm(outputs_grad.view(images.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
        "            perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "            perturbed_images = perturbed_images.clone().detach().requires_grad_(True).to(device)\n",
        "        else:\n",
        "            raise ValueError(\"No gradient computed for perturbed_images. The attack can't proceed.\")\n",
        "\n",
        "    return perturbed_images.detach()\n",
        "\n",
        "# Apply the modified DeepFool attack to the entire dataset and save the perturbed images\n",
        "batch_size = 64\n",
        "total_images = len(visualize_dataset)\n",
        "num_batches = (total_images + batch_size - 1) // batch_size\n",
        "\n",
        "for batch_idx in range(num_batches):\n",
        "    start_idx = batch_idx * batch_size\n",
        "    end_idx = min(start_idx + batch_size, total_images)\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = zip(*[visualize_dataset[i] for i in range(start_idx, end_idx)])\n",
        "    images = torch.stack(images).to(device)\n",
        "    labels = torch.tensor(labels).to(device)\n",
        "\n",
        "    # DeepFool attack for the batch of images\n",
        "    perturbed_images = deepfool_attack_batch(images, global_model, epsilon)\n",
        "\n",
        "    for j in range(end_idx - start_idx):\n",
        "        # Convert the perturbed image to a numpy array and change the channel order\n",
        "        perturbed_image_np = perturbed_images[j].squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "        # Convert the image to the BGR color format for saving\n",
        "        perturbed_image_bgr = cv2.cvtColor((perturbed_image_np * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Get the class label of the image\n",
        "        class_label = visualize_dataset.classes[labels[j]]\n",
        "\n",
        "        # Create a subdirectory for the class label if it does not exist\n",
        "        class_subdir = os.path.join(perturbed_dir, class_label)\n",
        "        os.makedirs(class_subdir, exist_ok=True)\n",
        "\n",
        "        # Save the perturbed image in the class subdirectory\n",
        "        perturbed_image_path = os.path.join(class_subdir, f\"perturbed_{start_idx + j}.jpg\")\n",
        "        cv2.imwrite(perturbed_image_path, perturbed_image_bgr)\n",
        "\n",
        "        # Undo normalization on the original image\n",
        "        original_image_np = images[j].squeeze().detach().cpu().numpy().transpose(1, 2, 0)\n",
        "        original_image_np = (original_image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
        "        original_image_np = original_image_np.astype(np.uint8)\n",
        "\n",
        "        # Convert the image to the BGR color format for visualization\n",
        "        original_image_bgr = cv2.cvtColor(original_image_np, cv2.COLOR_RGB2BGR)"
      ],
      "metadata": {
        "id": "zoN_NvvElsSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Dataset in Normal Machine Learning Environment"
      ],
      "metadata": {
        "id": "3QR9tAERmHDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/PinsData/105_classes_pins_dataset\"\n",
        "\n",
        "# Define the network architecture with Batch Normalization\n",
        "class AlexNetWithBN(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(AlexNetWithBN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(192),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the global model with Batch Normalization\n",
        "global_model = AlexNetWithBN().to(device)\n",
        "\n",
        "# Define the optimizer and loss function for the global model with weight decay\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Load the dataset\n",
        "dataset = ImageFolder(dataset_path, transform=transform)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoader objects for training and testing\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Training function with learning rate scheduling\n",
        "def train_model(model, criterion, optimizer, train_loader, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / len(train_loader.dataset) * 100\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Testing function\n",
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / len(test_loader.dataset) * 100\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Training and testing loop with learning rate scheduling\n",
        "num_epochs = 50\n",
        "train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], []\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_model(global_model, criterion, optimizer, train_loader, device, epoch)\n",
        "    test_loss, test_accuracy = test_model(global_model, criterion, test_loader, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Plot the training and testing accuracy and loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train')\n",
        "plt.plot(range(1, num_epochs+1), test_losses, label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Testing Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train')\n",
        "plt.plot(range(1, num_epochs+1), test_accuracies, label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Testing Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HE19e0hbmWwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Normal Machine Learning Environment"
      ],
      "metadata": {
        "id": "cHXoP2TvmpAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "dataset_path = \"/home/ec2-user/SageMaker/perturbed_images/\"\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Reduced data augmentation for the training set\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# No data augmentation for the testing set\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define datasets with proper transformations\n",
        "train_dataset = ImageFolder(dataset_path, transform=train_transform)\n",
        "test_dataset = ImageFolder(dataset_path, transform=test_transform)\n",
        "\n",
        "# Initialize the model with VGG16 and Dropout\n",
        "class VGG16WithDropout(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(VGG16WithDropout, self).__init__()\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        self.features = vgg16.features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Initialize the model with VGG16 and Dropout\n",
        "global_model = VGG16WithDropout().to(device)\n",
        "\n",
        "# Define the optimizer and loss function for the global model with weight decay\n",
        "learning_rate = 0.0001\n",
        "weight_decay = 1e-5\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Training function with learning rate scheduling and early stopping\n",
        "def train_model(model, criterion, optimizer, train_loader, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / len(train_loader.dataset) * 100\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Testing function\n",
        "def test_model(model, criterion, test_loader, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(test_loader.dataset)\n",
        "    epoch_accuracy = correct_predictions / len(test_loader.dataset) * 100\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "# Training and testing loop with learning rate scheduling and early stopping\n",
        "num_epochs = 30\n",
        "train_losses, train_accuracies, test_losses, test_accuracies = [], [], [], []\n",
        "\n",
        "best_test_accuracy = 0.0\n",
        "patience = 15\n",
        "early_stopping_counter = 0\n",
        "\n",
        "# Use ReduceLROnPlateau scheduler instead of StepLR\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "best_model_wts = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_accuracy = train_model(global_model, criterion, optimizer, train_loader, device, epoch)\n",
        "    test_loss, test_accuracy = test_model(global_model, criterion, test_loader, device)\n",
        "\n",
        "    # Use the scheduler\n",
        "    scheduler.step(test_accuracy)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Save the best model weights\n",
        "    if test_accuracy > best_test_accuracy:\n",
        "        best_test_accuracy = test_accuracy\n",
        "        best_model_wts = global_model.state_dict()\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "        if early_stopping_counter >= patience:\n",
        "            print(\"Early stopping...\")\n",
        "            break\n",
        "\n",
        "global_model.load_state_dict(best_model_wts)\n",
        "\n",
        "# Plot the training and testing accuracy and loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train')\n",
        "plt.plot(range(1, num_epochs+1), test_losses, label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Testing Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train')\n",
        "plt.plot(range(1, num_epochs+1), test_accuracies, label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Testing Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iqRfNXkWmrzh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}