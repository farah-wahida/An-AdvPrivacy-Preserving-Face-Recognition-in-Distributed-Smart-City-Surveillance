{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup"
      ],
      "metadata": {
        "id": "UQaS6FgQ5geC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Fvj5CC5ZkA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade tensorflow_federated"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Perturbation"
      ],
      "metadata": {
        "id": "DLk0zAVg5hC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform for the DataLoader used for visualization (with normalization)\n",
        "transform_visualize = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Download and load the CIFAR-10 dataset\n",
        "dataset_path = './CIFAR10_data'\n",
        "if not os.path.exists(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "\n",
        "# You can adjust the batch size and other parameters as needed\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(dataset_path, train=True, download=True, transform=transform_visualize),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the AlexNet architecture for CIFAR-10\n",
        "class AlexNetCIFAR10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNetCIFAR10, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize the global model\n",
        "global_model = AlexNetCIFAR10().to(device)\n",
        "\n",
        "# Define the optimizer and loss function for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Create a directory to save the perturbed images\n",
        "perturbed_dir = \"perturbed_images_cifar10\"\n",
        "os.makedirs(perturbed_dir, exist_ok=True)\n",
        "\n",
        "# Define the epsilon value for the DeepFool attack\n",
        "epsilon = 0.03\n",
        "\n",
        "# Modified DeepFool attack to increase perturbation\n",
        "def deepfool_attack_batch(images, model, epsilon, reduction_factor=0.95):\n",
        "    model.eval()\n",
        "    perturbed_images = images.clone().detach().requires_grad_(True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        original_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    outputs_grad = None\n",
        "    while True:\n",
        "        outputs = model(perturbed_images)\n",
        "        predicted_labels = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        if torch.all(predicted_labels == original_labels):\n",
        "            break\n",
        "\n",
        "        if perturbed_images.grad is not None:\n",
        "            perturbed_images.grad.data.zero_()\n",
        "\n",
        "        loss = criterion(outputs, original_labels)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        if perturbed_images.grad is not None and outputs_grad is None:\n",
        "            outputs_grad = perturbed_images.grad.data.clone()\n",
        "\n",
        "        if outputs_grad is not None:\n",
        "            perturbation = torch.abs(loss) / torch.norm(outputs_grad.view(images.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
        "            perturbed_images = perturbed_images + perturbation * outputs_grad / torch.norm(outputs_grad.view(images.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
        "            perturbed_images = torch.clamp(perturbed_images, 0, 1)\n",
        "            perturbed_images = perturbed_images.clone().detach().requires_grad_(True).to(device)\n",
        "        else:\n",
        "            raise ValueError(\"No gradient computed for perturbed_images. The attack can't proceed.\")\n",
        "\n",
        "    return perturbed_images.detach()\n",
        "\n",
        "# Apply the modified DeepFool attack to the entire dataset and save the perturbed images\n",
        "total_images = len(train_loader.dataset)\n",
        "num_batches = (total_images + batch_size - 1) // batch_size\n",
        "\n",
        "for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # DeepFool attack for the batch of images\n",
        "    perturbed_images = deepfool_attack_batch(images, global_model, epsilon)\n",
        "\n",
        "    # Calculate the number of images in this batch\n",
        "    num_images_in_batch = len(images)\n",
        "\n",
        "    for j in range(num_images_in_batch):\n",
        "        # Convert the perturbed image to a numpy array\n",
        "        perturbed_image_np = perturbed_images[j].detach().cpu().numpy().transpose((1, 2, 0))\n",
        "\n",
        "        # Get the class label of the image\n",
        "        class_label = labels[j].item()\n",
        "\n",
        "        # Create a subdirectory for the class label if it does not exist\n",
        "        class_subdir = os.path.join(perturbed_dir, str(class_label))\n",
        "        os.makedirs(class_subdir, exist_ok=True)\n",
        "\n",
        "        # Save the perturbed image\n",
        "        perturbed_image_path = os.path.join(class_subdir, f\"perturbed_{batch_idx * num_images_in_batch + j}.jpg\")\n",
        "        perturbed_image_np = (perturbed_image_np * 255).astype(np.uint8)\n",
        "        cv2.imwrite(perturbed_image_path, cv2.cvtColor(perturbed_image_np, cv2.COLOR_RGB2BGR))\n"
      ],
      "metadata": {
        "id": "hwwTKpvB5hbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbed Dataset in Federated Learning Environment (using GoogleNet model)"
      ],
      "metadata": {
        "id": "cEBRi7cP5hvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataset import Subset\n",
        "from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transform to apply to the images\n",
        "perturbed_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Specify the path to the dataset directory\n",
        "perturbed_dataset_path = \"./perturbed_images_cifar10/\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "if not os.path.exists(perturbed_dataset_path):\n",
        "    raise FileNotFoundError(f\"The directory {perturbed_dataset_path} does not exist.\")\n",
        "\n",
        "# Define the network architecture (GoogLeNet)\n",
        "class GoogLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=105):\n",
        "        super(GoogLeNet, self).__init__()\n",
        "        # Load the pretrained GoogLeNet model\n",
        "        self.model = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        # Modify the final fully connected layer to match the number of classes\n",
        "        self.model.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Wrap the model with DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs for Data Parallelism.\")\n",
        "    global_model = nn.DataParallel(GoogLeNet()).to(device)\n",
        "else:\n",
        "    global_model = GoogLeNet().to(device)\n",
        "\n",
        "# Define the federated learning parameters\n",
        "num_clients = 4\n",
        "fraction = 0.2\n",
        "num_epochs = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Define the optimizer and learning rate scheduler for the global model\n",
        "optimizer = optim.Adam(global_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Divide the perturbed dataset into train and test sets\n",
        "perturbed_full_dataset = ImageFolder(perturbed_dataset_path, transform=perturbed_transform)\n",
        "perturbed_num_samples = len(perturbed_full_dataset)\n",
        "perturbed_num_train_samples = int(perturbed_num_samples * (1 - fraction))\n",
        "perturbed_num_test_samples = perturbed_num_samples - perturbed_num_train_samples\n",
        "perturbed_train_dataset, perturbed_test_dataset = random_split(perturbed_full_dataset, [perturbed_num_train_samples, perturbed_num_test_samples])\n",
        "\n",
        "# Split the train dataset into client datasets\n",
        "perturbed_train_indices = list(range(len(perturbed_train_dataset)))\n",
        "perturbed_client_datasets = []\n",
        "start = 0\n",
        "for _ in range(num_clients):\n",
        "    end = start + int(len(perturbed_train_indices) / num_clients)\n",
        "    indices = perturbed_train_indices[start:end]\n",
        "    subset = Subset(perturbed_train_dataset, indices)\n",
        "    perturbed_client_datasets.append(subset)\n",
        "    start = end\n",
        "\n",
        "# Function to train a local model on a client's dataset\n",
        "def train_local_model(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = correct / total * 100\n",
        "    return model.state_dict(), train_loss, train_accuracy\n",
        "\n",
        "# Function to aggregate model updates using federated averaging\n",
        "def aggregate_models(global_model, local_models):\n",
        "    global_dict = global_model.state_dict()\n",
        "    for key in global_dict.keys():\n",
        "        # Convert model weights to float\n",
        "        global_dict[key] = torch.stack([local_models[i][key].float() for i in range(len(local_models))], dim=0).mean(0)\n",
        "    global_model.load_state_dict(global_dict)\n",
        "    return global_model\n",
        "\n",
        "# Lists to store the values for plotting\n",
        "train_losses_plot = []\n",
        "train_accuracies_plot = []\n",
        "test_losses_plot = []\n",
        "test_accuracies_plot = []\n",
        "\n",
        "# Train the global model using federated learning\n",
        "for epoch in range(num_epochs):\n",
        "    local_models = []\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "\n",
        "    # Train local models on client datasets\n",
        "    for client_dataset in perturbed_client_datasets:\n",
        "        train_loader = DataLoader(client_dataset, batch_size=32, shuffle=True)\n",
        "        local_model = GoogLeNet().to(device)\n",
        "        # Wrap the local model with DataParallel if multiple GPUs are available\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            local_model = nn.DataParallel(local_model)\n",
        "\n",
        "        local_model.load_state_dict(global_model.state_dict())\n",
        "        local_optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Learning rate scheduler for local models\n",
        "        local_scheduler = optim.lr_scheduler.StepLR(local_optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "        local_model_dict, train_loss, train_accuracy = train_local_model(local_model, train_loader, local_optimizer, criterion, device)\n",
        "        local_models.append(local_model_dict)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        local_scheduler.step()\n",
        "\n",
        "    # Aggregate the local models using federated averaging\n",
        "    global_model = aggregate_models(global_model, local_models)\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    test_loader = DataLoader(perturbed_test_dataset, batch_size=32, shuffle=False)\n",
        "    global_model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = global_model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    test_accuracy = test_correct / test_total * 100\n",
        "\n",
        "    # Append values to the plotting lists\n",
        "    train_losses_plot.append(sum(train_losses) / len(train_losses))\n",
        "    train_accuracies_plot.append(sum(train_accuracies) / len(train_accuracies))\n",
        "    test_losses_plot.append(test_loss)\n",
        "    test_accuracies_plot.append(test_accuracy)\n",
        "\n",
        "    # Print the epoch statistics\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Train Loss: {sum(train_losses) / len(train_losses):.4f} | Train Accuracy: {sum(train_accuracies) / len(train_accuracies):.2f}%\")\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print()\n",
        "\n",
        "# Plotting train and test loss\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses_plot, label='Train Loss')\n",
        "plt.plot(test_losses_plot, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plotting train and test accuracy\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies_plot, label='Train Accuracy')\n",
        "plt.plot(test_accuracies_plot, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wo4JPiRC5iMD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}